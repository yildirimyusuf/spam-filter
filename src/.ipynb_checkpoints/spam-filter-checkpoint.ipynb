{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_XI9fvRthZT"
   },
   "source": [
    "# Giriş\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FcXRMoxxkfH"
   },
   "source": [
    "Bu projede, ling-spam veri seti üzerinde Naïve Bayes ve SVM tabanlı bir sınıflandırma kullanarak bir e-posta spam filtresi tasarlayacağız.\n",
    "\n",
    "E-posta spam filtrelerinin bir listesi:\n",
    "* Naive Bayes Classifier\n",
    "  * Bernoulli NB classiﬁer with    binary features\n",
    "  * Multinomial NB with binary features\n",
    "  * Multinomial NB with term frequency (TF) features \n",
    "* SVM based Classifier\n",
    "* Adversarial Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0FkO9lhtprs"
   },
   "source": [
    "# Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgGAEo97xrAO"
   },
   "source": [
    "Ling-spam corpus, Linguist eposta listesi yasal veya istenmeyen e-postalar olarak sınıflandırılan e-postaları içerir. Korpus, lemmatizasyonlu/olmayan ve stop-word kaldırmalı/kaldırmasız önceden işlenmiş aynı e-postaları içeren dört alt klasöre bölünmüştür. Her alt klasördeki e-postalar 10 kısma bölünmüştür.\n",
    "\n",
    "Bu projede, ling-spam copus ilk 9 kısmını **eğitim verileri** olarak ve 10. kısmı **test verileri** olarak kullanacağız. Ayrıca corpus hem lemmatizasyon hem de stop-word etkinken kullanacağız (**lemm_stop** klasörü altında).\n",
    "\n",
    "Tüm ling-spam veri kümesini buradan indirin: http://www.aueb.gr/users/ion/data/lingspam_public.tar.gz\n",
    "\n",
    "Deneylere başlamadan önce, corpus üzerinde bazı ön işlemler yapmamız gerekiyor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V28eTt1E7wt3"
   },
   "source": [
    "## Veri Ön İşleme\n",
    "\n",
    "Ön işlemenin amacı, corpus temiz ve birleşik bir biçimde düzenlemek ve sonraki deneyleri kolaylaştırmaktır. Bu adımda, lingspam corpus'a (lemm_stop klasörü) dayalı 2 csv dosyası **lingspam_train.csv** ve **lingspam_test.csv** oluşturacağım. İlk 9 kısım eğitim setini, 10. kısım ise test setini oluşturmaktadır.\n",
    "\n",
    "Csv dosyasında, e-posta konusu, e-posta gövdesi, e-posta spam'i veya yasal gibi ihtiyacımız olan tüm bilgileri içerir.\n",
    "\n",
    "Veri ön işleme yapmak için kullandığım python betiği:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HTZBKnVOCScq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import codecs\n",
    "import string\n",
    "\n",
    "\n",
    "TRAINSET_PATH = '../data/train/'\n",
    "TESTSET_PATH = '../data/test/'\n",
    "LINGSPAM_TRAIN_CSV_PATH = TRAINSET_PATH + 'lingspam_train.csv'\n",
    "LINGSPAM_TEST_CSV_PATH = TESTSET_PATH + 'lingspam_test.csv'\n",
    "\n",
    "\n",
    "def generate_trainset(input_dir, output_path):\n",
    "    l = []\n",
    "\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        path = root.split(os.sep)\n",
    "        part_name = os.path.basename(root)\n",
    "        for file in files:\n",
    "            if not file.endswith('.txt'):\n",
    "                continue\n",
    "\n",
    "            d = {}\n",
    "            file_name = file.replace('.txt', '')\n",
    "            file_path = os.path.join(root, file)\n",
    "            \n",
    "            with codecs.open(file_path, mode='r', encoding='utf8', errors='ignore') as f:\n",
    "                line_counter = 0\n",
    "                for line in f.readlines():\n",
    "                    line = line.strip()\n",
    "                    if line_counter == 0:   # subject\n",
    "                        subject = line.replace('Subject:', '').strip()\n",
    "                    if line_counter == 2:\n",
    "                        email = line\n",
    "                    line_counter += 1\n",
    "            d['email_subject'] = subject\n",
    "            d['email_body'] = email\n",
    "            d['part_name'] = part_name\n",
    "            d['file_name'] = file_name\n",
    "            d['is_spam'] = 1 if file_name.startswith('spmsg') else 0\n",
    "            l.append(d)\n",
    "    \n",
    "    with codecs.open(output_path, mode='w', encoding='utf8', errors='ignore') as out_file:\n",
    "        writer = csv.DictWriter(out_file, l[0].keys())\n",
    "        writer.writeheader()\n",
    "        for row in l:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "# generate_trainset(TRAINSET_PATH, LINGSPAM_TRAIN_CSV_PATH)\n",
    "# generate_trainset(TESTSET_PATH, LINGSPAM_TEST_CSV_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5ttlnQxCePW"
   },
   "source": [
    "Veri kümesi klasörlerini colab'a yüklemek zor olduğundan yukarıdaki kodu bu not defterinde gerçekten çalıştıramıyoruz. Bu betiği çevrimdışı çalıştırdım ve 2 beklenen csv dosyası oluşturdum. linki buraya koyuyorum\n",
    "\n",
    "* [lingspam_train.csv](https://drive.google.com/file/d/14dLhYauFUwm7a8bonGr_8MoQRCy_ufpA/view?usp=sharing)\n",
    "* [lingspan_test.csv](https://drive.google.com/file/d/1R4PgItvVQ-pZ3IES2YDSFI5wGxLUZmz_/view?usp=sharing)\n",
    "\n",
    "Lütfen bu 2 dosyayı indirin ve bu colab not defterine yükleyin, soldaki Dosyalar sekmesindeki yükle düğmesine tıklayın.\n",
    "\n",
    "Şimdi corpusu temizledik, Pandas kullanarak onlara bir göz atalım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "id": "06NadHmFDkmU",
    "outputId": "75a94ba2-0ecd-4d23-8b09-0753f32a078f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset column names:\n",
      "email_subject\n",
      "email_body\n",
      "part_name\n",
      "file_name\n",
      "is_spam\n",
      "\n",
      "lingspam trainset:\n",
      "                                          email_subject  \\\n",
      "0                  job post - apple-iss research center   \n",
      "1                                                   NaN   \n",
      "2          query : letter frequency text identification   \n",
      "3                                                  risk   \n",
      "4                              request book information   \n",
      "...                                                 ...   \n",
      "2597                            love profile - ysuolvpv   \n",
      "2598                                    ask join kiddin   \n",
      "2599                      anglicization composer ' name   \n",
      "2600  re : 6 . 797 , comparative method : n - ary co...   \n",
      "2601                 re : american - english australium   \n",
      "\n",
      "                                             email_body part_name   file_name  \\\n",
      "0     content - length : 3386 apple-iss research cen...     part3   6-110msg3   \n",
      "1     lang classification grime , joseph e . barbara...     part3   6-126msg1   \n",
      "2     post inquiry sergeus atama ( satama @ umabnet ...     part3  6-1125msg2   \n",
      "3     colleague research differ degree risk perceive...     part3  6-1157msg2   \n",
      "4     earlier morn phone friend mine live south amer...     part3  6-1147msg2   \n",
      "...                                                 ...       ...         ...   \n",
      "2597  hello thank stop ! ! many pic hot video ! most...     part8   spmsgc134   \n",
      "2598  list owner : \" kiddin \" invite join mail list ...     part8   spmsgc108   \n",
      "2599  judge return post , must sound kind self-procl...     part8  8-1119msg1   \n",
      "2600  gotcha ! two separate fallacy argument against...     part8   6-825msg3   \n",
      "2601  hello ! ' m work thesis concern attitude towar...     part8   6-813msg1   \n",
      "\n",
      "      is_spam  \n",
      "0           0  \n",
      "1           0  \n",
      "2           0  \n",
      "3           0  \n",
      "4           0  \n",
      "...       ...  \n",
      "2597        1  \n",
      "2598        1  \n",
      "2599        0  \n",
      "2600        0  \n",
      "2601        0  \n",
      "\n",
      "[2602 rows x 5 columns]\n",
      "\n",
      "lingspam testset:\n",
      "                                         email_subject  \\\n",
      "0                            semitic language workshop   \n",
      "1    endanger language - edinburgh , sept 98 - call...   \n",
      "2                   workshop sposs preliminary program   \n",
      "3                               nels 29 - - call paper   \n",
      "4                                   kornfilt : turkish   \n",
      "..                                                 ...   \n",
      "286                           sla conference pari 1999   \n",
      "287                            conference announcement   \n",
      "288                                           semantic   \n",
      "289                          honor two keynote speaker   \n",
      "290                                           typology   \n",
      "\n",
      "                                            email_body part_name  file_name  \\\n",
      "0    workshop computational approach semitic langua...    part10  9-920msg1   \n",
      "1    endanger language - role specialist ? - - - - ...    part10  9-858msg1   \n",
      "2    sposs sound pattern spontaneous speech product...    part10  9-989msg1   \n",
      "3    * * * * * * * * * * * * n e l s 29 * * * * * *...    part10  9-697msg2   \n",
      "4    jaklin kornfilt ( 1997 ) , turkish . london yo...    part10  9-645msg1   \n",
      "..                                                 ...       ...        ...   \n",
      "286  call papers xi th international conference \" a...    part10  9-640msg1   \n",
      "287  southern illinoi university edwardsville carbo...    part10  9-738msg1   \n",
      "288  bring attention two publication john benjamin ...    part10  9-886msg1   \n",
      "289  international conference natural language proc...    part10  9-947msg1   \n",
      "290  bring attention recent publication john benjam...    part10  9-957msg1   \n",
      "\n",
      "     is_spam  \n",
      "0          0  \n",
      "1          0  \n",
      "2          0  \n",
      "3          0  \n",
      "4          0  \n",
      "..       ...  \n",
      "286        0  \n",
      "287        0  \n",
      "288        0  \n",
      "289        0  \n",
      "290        0  \n",
      "\n",
      "[291 rows x 5 columns]\n",
      "\n",
      "Trainset size: 2602\n",
      "Testset size: 291\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "lingspam_train_path = './lingspam_train.csv'\n",
    "lingspam_test_path = './lingspam_test.csv'\n",
    "dtype={\n",
    "    'email_subject': str,\n",
    "    'email_body': str,\n",
    "    'part_name': str,\n",
    "    'file_name':str,\n",
    "    'is_spam': int\n",
    "    }\n",
    "\n",
    "lingspam_train_df = pd.read_csv(lingspam_train_path, dtype=dtype)\n",
    "lingspam_test_df = pd.read_csv(lingspam_test_path, dtype=dtype)\n",
    "\n",
    "print(\"Dataset column names:\")\n",
    "for col in lingspam_train_df.columns:\n",
    "  print(col)\n",
    "\n",
    "print('\\nlingspam trainset:')\n",
    "print(lingspam_train_df)\n",
    "print('\\nlingspam testset:')\n",
    "print(lingspam_test_df)\n",
    "\n",
    "trainset_size = lingspam_train_df.shape[0]\n",
    "testset_size = lingspam_test_df.shape[0]\n",
    "print(\"\\nTrainset size: \" + str(trainset_size))\n",
    "print(\"Testset size: \" + str(testset_size))\n",
    "\n",
    "y_train = lingspam_train_df['is_spam'].to_numpy()\n",
    "y_test = lingspam_test_df['is_spam'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvX8qtE-H4T0"
   },
   "source": [
    "CSV veri çerçevelerinin şeklinden, train setinde 2602 örnek, test setinde 291 test durumu vardır.\n",
    "\n",
    "Daha sonra kelime sıklığını saymak için **lingspam_train.csv** train setinden bir sözlük yapacağız. Bu sözlükle, train setinden ve test setinden ikili özellikleri veya terim frekans özelliklerini kolayca çıkarabiliriz.\n",
    "\n",
    "Eamil gövdesi için, postanın spam olup olmadığını etkilemediklerini düşündüğüm için tüm noktalama işaretlerini kaldırıyorum. Ayrıca uzunluğu 2'den az olan kelimeler kaldırılmıştır çünkü tek harflerin gerçek kelimeler olduğunu düşünmüyorum. Ayrıca, sadece tüm karakterleri alfabe harfleri (a-z) olan kelimeleri saklıyorum.\n",
    "\n",
    "Özellik seçme adımı için, çok fazla hesaplama maliyeti getireceği için özelliklerimizin boyutunun çok büyük olmasını istemiyoruz. Lingspam eğitim veri setinde yaklaşık 55 bin benzersiz kelime var, bilgi kazanımına dayalı olarak bazı özellik seçimleri yapacağız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "JoZ4Z79o28Eo",
    "outputId": "86d8ef9f-f75d-478e-de75-99d16998abc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique words in lingspam trainset: 44664\n",
      "The total times they appeared: 660163\n",
      "The 20 most common words in trainset:\n",
      "('language', 7259)\n",
      "('university', 5271)\n",
      "('linguistic', 2890)\n",
      "('de', 2849)\n",
      "('address', 2778)\n",
      "('one', 2702)\n",
      "('information', 2646)\n",
      "('send', 2232)\n",
      "('order', 2206)\n",
      "('conference', 2131)\n",
      "('work', 2056)\n",
      "('english', 2052)\n",
      "('please', 2018)\n",
      "('include', 1995)\n",
      "('mail', 1985)\n",
      "('email', 1956)\n",
      "('program', 1895)\n",
      "('name', 1793)\n",
      "('http', 1786)\n",
      "('paper', 1770)\n",
      "\n",
      "The length of current dictionary: 44664\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "words = []\n",
    "dictionary = {}\n",
    "\n",
    "for index, row in lingspam_train_df.iterrows():\n",
    "  email = row['email_body'].split(' ')\n",
    "  email = [word for word in email if word not in string.punctuation]\n",
    "  email = [word for word in email if len(word) > 1]\n",
    "  email = [word for word in email if word.isalpha() == True]\n",
    "  words += email\n",
    "\n",
    "dictionary = Counter(words)\n",
    "\n",
    "unique_num = len(dictionary)\n",
    "total_num = sum(dictionary.values())\n",
    "\n",
    "print(\"The number of unique words in lingspam trainset: \" + str(unique_num))\n",
    "print(\"The total times they appeared: \" + str(total_num))\n",
    "\n",
    "print(\"The 20 most common words in trainset:\")\n",
    "print(*dictionary.most_common(20), sep='\\n')\n",
    "\n",
    "print('\\nThe length of current dictionary: ' + str(len(dictionary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgTx4XpA81ui"
   },
   "source": [
    "# Deneyler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlD_mFgw8B6g"
   },
   "source": [
    "## Öznitelik Seçimi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kw7Abkvu_8I6"
   },
   "source": [
    "Sonraki 3 Naive Bayes sınıflandırıcısında, özellikleri kodlamak için 2 yol kullanacağız:\n",
    "\n",
    "* Binary/Boolean Features\n",
    "  * $x_i=1$ if term $i$ appears in a doc; $0$ otherwise\n",
    "  * Each doc has a $M$ dimension boolean features, $x = (x_1, x_2, ..., x_M)$ \n",
    "* Term Frequencies (TF) Features\n",
    "  * $x_i$: number of times term $i$ appears in a doc\n",
    "  * Each doc is represented by $x = (x_1, x_2, ... x_M)$, a vector of term frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BL2IjHn83eSK"
   },
   "source": [
    "**Bilgi kazancı** (IG), özellik seçimini (sıralama özellikleri) gerçekleştirmek için metrik olarak kullanılacağız. Burada IG metriği, veri kümesinde yalnızca terimlerin ortaya çıkışını (ve terimlerin görünme sıklığını değil) açıklar. Ayrıca, sıfır olasılığa sahip bir olay olmadığından emin olmak için **Laplacian Smoothing** kullanıyoruz.\n",
    "\n",
    "Şimdi, eğitim veri kümesindeki kelimeler için [IG](https://drive.google.com/file/d/1jLnCu_e7dmMIhLXgTLkRemo3U6rvZ_kc/view?usp=sharing) hesaplayalım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "Wi-sVzmS4KIc",
    "outputId": "353f10a3-e1d5-4641-b233-e2f0df0b7189"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total legit email number = 2170.0\n",
      "Total spam email number = 432.0\n",
      "p = 0.8339738662567256\n",
      "H(C) = 0.6485330171848535\n",
      "word: language, info_gain: 0.19967490044495856\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "total_legit_emails = 0.0\n",
    "total_spam_emails = 0.0\n",
    "\n",
    "for index, row in lingspam_train_df.iterrows():\n",
    "  is_spam = row['is_spam']\n",
    "  if is_spam == 1:\n",
    "    total_spam_emails += 1\n",
    "  else:\n",
    "    total_legit_emails += 1\n",
    "\n",
    "print(\"Total legit email number = {}\".format(total_legit_emails))\n",
    "print(\"Total spam email number = {}\".format(total_spam_emails))\n",
    "\n",
    "p = total_legit_emails / (total_spam_emails + total_legit_emails)\n",
    "print(\"p = {}\".format(p))\n",
    "\n",
    "h_c = -1 * p * math.log(p, 2) - (1 - p) * math.log(1 - p, 2)\n",
    "print(\"H(C) = {}\".format(h_c))\n",
    "\n",
    "\n",
    "def count_legit_emails_with_word(word):\n",
    "  num_legit_emails_with_word = 0\n",
    "  for index, row in lingspam_train_df.iterrows():\n",
    "    if row['is_spam'] == 0 and word in row['email_body'].split(' '):\n",
    "      num_legit_emails_with_word += 1\n",
    "  return num_legit_emails_with_word\n",
    "      \n",
    "def count_spam_emails_with_word(word):\n",
    "  num_spam_emails_with_word = 0\n",
    "  for index, row in lingspam_train_df.iterrows():\n",
    "    if row['is_spam'] == 1 and word in row['email_body'].split(' '):\n",
    "      num_spam_emails_with_word += 1\n",
    "  return num_spam_emails_with_word\n",
    "\n",
    "def h_legit_word_not_present(word):\n",
    "  num_legit_emails_with_word = count_legit_emails_with_word(word)\n",
    "  num_spam_emails_with_word = count_spam_emails_with_word(word)\n",
    "  return (total_legit_emails - num_legit_emails_with_word) / (total_spam_emails + total_legit_emails) * math.log((total_legit_emails - num_legit_emails_with_word) / (total_spam_emails - num_spam_emails_with_word + total_legit_emails - num_legit_emails_with_word), 2)\n",
    "\n",
    "def h_spam_word_not_present(word):\n",
    "  num_legit_emails_with_word = count_legit_emails_with_word(word)\n",
    "  num_spam_emails_with_word = count_spam_emails_with_word(word)\n",
    "  return (total_spam_emails - num_spam_emails_with_word) / (total_spam_emails + total_legit_emails) * math.log((total_spam_emails - num_spam_emails_with_word) / (total_spam_emails - num_spam_emails_with_word + total_legit_emails - num_legit_emails_with_word), 2)\n",
    "\n",
    "def h_legit_word_is_present(word):\n",
    "  num_legit_emails_with_word = count_legit_emails_with_word(word)\n",
    "  num_spam_emails_with_word = count_spam_emails_with_word(word)\n",
    "  return num_legit_emails_with_word / (total_spam_emails + total_legit_emails) * math.log(num_legit_emails_with_word / (num_spam_emails_with_word + num_legit_emails_with_word), 2)\n",
    "\n",
    "def h_spam_word_is_present(word):\n",
    "  num_legit_emails_with_word = count_legit_emails_with_word(word)\n",
    "  num_spam_emails_with_word = count_spam_emails_with_word(word)\n",
    "  return num_spam_emails_with_word / (total_spam_emails + total_legit_emails) * math.log(num_spam_emails_with_word / (num_spam_emails_with_word + num_legit_emails_with_word), 2)\n",
    "\n",
    "def info_gain(word):\n",
    "  h_c_x = -1 * (h_legit_word_not_present(word) + h_spam_word_not_present(word) + h_legit_word_is_present(word) + h_spam_word_is_present(word))\n",
    "  ig = h_c - h_c_x\n",
    "  return ig\n",
    "\n",
    "word = \"language\"\n",
    "print(\"word: {}, info_gain: {}\".format(word, info_gain(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quDxLxZ-sBMV"
   },
   "source": [
    "Veri kümesindeki tüm kelimelerin (45k kelime!) hesaplama süresi oldukça uzun olduğu için, hesaplamayı çevrimdışı yaptım ve bilgi kazanç değerleri içeren sözlüğü bir csv dosyasına kaydettim. Bu dosyayı bu [link](https://drive.google.com/file/d/1aCKYVrZ-q4shiXyAfSxqZ06cMLEzwDn0/view?usp=sharing) indirebilirsiniz. İndirdikten sonra bu colab defterine yükleyin ve sonraki deneylerde buna ihtiyacımız olacak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "dQDswlW8swmC",
    "outputId": "98414507-3d9e-4fd9-93e8-3c95e238b697"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Information Gain dictionary:\n",
      "             word  freq        ig\n",
      "0        language  7259  0.199639\n",
      "1      university  5271  0.144992\n",
      "2      linguistic  2890  0.145682\n",
      "3              de  2849  0.049473\n",
      "4         address  2778  0.007449\n",
      "...           ...   ...       ...\n",
      "44659    confusio     1 -0.000124\n",
      "44660      mathce     1 -0.000124\n",
      "44661  pharmacist     1 -0.000124\n",
      "44662       lolly     1 -0.000124\n",
      "44663     jessica     1 -0.000124\n",
      "\n",
      "[44664 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "ig_filepath = './ig.csv'\n",
    "\n",
    "dtype={\n",
    "    'word': str,\n",
    "    'freq': int,\n",
    "    'ig': float,\n",
    "    }\n",
    "\n",
    "ig_df = pd.read_csv(ig_filepath, dtype=dtype)\n",
    "\n",
    "\n",
    "print('\\nInformation Gain dictionary:')\n",
    "print(ig_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_LtVupAqRGjE"
   },
   "source": [
    "Artık veri setindeki her kelime için Bilgi Kazanımı değerleri alıyoruz, kelimeyi IG'ye göre azalan düzende sıralayarak özellik seçimi yapabiliyoruz. Eğitim verilerinden, en yüksek Bilgi Kazanımı (IG) puanlarına göre en iyi N özelliği ($N = {10, 100, 1000}$) seçin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "klPSo5v6x0Au",
    "outputId": "879c4053-0970-486f-c657-06a4a0d1a8d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dictionary sorted by IG on desending order:\n",
      "                word  freq        ig\n",
      "0           language  7259  0.199639\n",
      "239           remove   437  0.168905\n",
      "48              free  1017  0.159972\n",
      "2         linguistic  2890  0.145682\n",
      "1         university  5271  0.144992\n",
      "...              ...   ...       ...\n",
      "29422        loveday     1 -0.000124\n",
      "29421  transferencia     1 -0.000124\n",
      "29420         douaud     1 -0.000124\n",
      "29419          accra     1 -0.000124\n",
      "44663        jessica     1 -0.000124\n",
      "\n",
      "[44664 rows x 3 columns]\n",
      "\n",
      "Top-10 features:\n",
      "           word  freq        ig\n",
      "0      language  7259  0.199639\n",
      "239      remove   437  0.168905\n",
      "48         free  1017  0.159972\n",
      "2    linguistic  2890  0.145682\n",
      "1    university  5271  0.144992\n",
      "64        money   916  0.118676\n",
      "529       click   251  0.101160\n",
      "186      market   509  0.091497\n",
      "22          our  1707  0.087068\n",
      "87     business   809  0.083016\n",
      "\n",
      "Top-100 features:\n",
      "            word  freq        ig\n",
      "0       language  7259  0.199639\n",
      "239       remove   437  0.168905\n",
      "48          free  1017  0.159972\n",
      "2     linguistic  2890  0.145682\n",
      "1     university  5271  0.144992\n",
      "...          ...   ...       ...\n",
      "40         study  1232  0.035863\n",
      "27      workshop  1613  0.035568\n",
      "1318    security    91  0.035187\n",
      "66      analysis   905  0.034641\n",
      "467          off   277  0.034402\n",
      "\n",
      "[100 rows x 3 columns]\n",
      "\n",
      "Top-1000 features:\n",
      "            word  freq        ig\n",
      "0       language  7259  0.199639\n",
      "239       remove   437  0.168905\n",
      "48          free  1017  0.159972\n",
      "2     linguistic  2890  0.145682\n",
      "1     university  5271  0.144992\n",
      "...          ...   ...       ...\n",
      "532       effect   249  0.006954\n",
      "599        north   221  0.006920\n",
      "1588   beautiful    72  0.006910\n",
      "1494        self    77  0.006910\n",
      "757          dan   172  0.006901\n",
      "\n",
      "[1000 rows x 3 columns]\n",
      "\n",
      "Top-10 words:\n",
      "['language', 'remove', 'free', 'linguistic', 'university', 'money', 'click', 'market', 'our', 'business']\n",
      "\n",
      "Top-100 words:\n",
      "['language', 'remove', 'free', 'linguistic', 'university', 'money', 'click', 'market', 'our', 'business', 'today', 'product', 'advertise', 'company', 'sell', 'english', 'linguistics', 'million', 'income', 'internet', 'day', 'guarantee', 'thousand', 'save', 'easy', 'over', 'best', 'purchase', 'win', 'check', 'buy', 'bulk', 'want', 'cash', 'dollar', 'cost', 'every', 'edu', 'll', 'mailing', 'service', 'com', 'yourself', 'papers', 'hour', 'linguist', 'hundred', 'theory', 'earn', 'profit', 'customer', 'de', 'abstract', 'success', 'fun', 'offer', 'month', 'yours', 'conference', 'receive', 'ever', 'watch', 'speaker', 'bonus', 'mail', 'discussion', 'credit', 'zip', 'here', 'pay', 'live', 'amaze', 'sale', 'syntax', 'department', 'start', 'investment', 'online', 'anywhere', 'toll', 'grammar', 'dream', 'huge', 'financial', 'science', 'ad', 'deadline', 'simply', 'week', 'structure', 'friend', 'mlm', 'need', 'wait', 'fresh', 'study', 'workshop', 'security', 'analysis', 'off']\n",
      "\n",
      "Top-1000 words:\n",
      "['language', 'remove', 'free', 'linguistic', 'university', 'money', 'click', 'market', 'our', 'business', 'today', 'product', 'advertise', 'company', 'sell', 'english', 'linguistics', 'million', 'income', 'internet', 'day', 'guarantee', 'thousand', 'save', 'easy', 'over', 'best', 'purchase', 'win', 'check', 'buy', 'bulk', 'want', 'cash', 'dollar', 'cost', 'every', 'edu', 'll', 'mailing', 'service', 'com', 'yourself', 'papers', 'hour', 'linguist', 'hundred', 'theory', 'earn', 'profit', 'customer', 'de', 'abstract', 'success', 'fun', 'offer', 'month', 'yours', 'conference', 'receive', 'ever', 'watch', 'speaker', 'bonus', 'mail', 'discussion', 'credit', 'zip', 'here', 'pay', 'live', 'amaze', 'sale', 'syntax', 'department', 'start', 'investment', 'online', 'anywhere', 'toll', 'grammar', 'dream', 'huge', 'financial', 'science', 'ad', 'deadline', 'simply', 'week', 'structure', 'friend', 'mlm', 'need', 'wait', 'fresh', 'study', 'workshop', 'security', 'analysis', 'off', 'marketing', 'ship', 'topic', 'cd', 'instruction', 'fantastic', 'profitable', 'package', 'back', 're', 'legal', 'excite', 'spend', 'net', 'research', 'step', 'site', 'freedom', 'issue', 'committee', 'debt', 'price', 'discourse', 'immediately', 'xxx', 'development', 'delete', 'advertisement', 'keep', 'everything', 'aol', 'semantic', 'overnight', 'reference', 'line', 'delivery', 'again', 'right', 'reply', 'orders', 'sales', 'spam', 'home', 'never', 'secret', 'theoretical', 'simple', 'fill', 'hit', 'resell', 'speech', 'enter', 'ac', 'author', 'once', 'stealth', 'tax', 'next', 'adult', 'tel', 'risk', 'down', 'acquisition', 'unlimit', 'list', 'amount', 'campaign', 'phonology', 'message', 'card', 'computational', 'bill', 'own', 'aspect', 'partner', 'cognitive', 'don', 'monthly', 'bank', 'amazing', 'plans', 'hottest', 'undeliverable', 'video', 'lexical', 'submission', 'visa', 've', 'absolutely', 'love', 'super', 'return', 'semantics', 'personal', 'fax', 'works', 'invest', 'student', 'opportunity', 'lottery', 'hello', 'big', 'context', 'capital', 'choose', 'hot', 'weekly', 'french', 'engine', 'great', 'per', 'car', 'life', 'is', 'add', 'relax', 'join', 'prove', 'affiliation', 'reports', 'remember', 'between', 'top', 'effective', 'institute', 'guaranteed', 'hundreds', 'hobby', 'focus', 'nothing', 'us', 'syntactic', 'tell', 'exactly', 'construction', 'luck', 'brand', 'verb', 'try', 'cent', 'refund', 'retire', 'generate', 'extra', 'total', 'comply', 'quick', 'envelope', 'put', 'translation', 'corporations', 'everythe', 'millions', 'dollars', 'worldwide', 'morphology', 'decide', 'lose', 'fortune', 'rate', 'federal', 'faster', 'totally', 'future', 'april', 'lists', 'duplicate', 'everyone', 'practically', 'german', 'role', 'seven', 'paper', 'sources', 'pp', 'greatest', 'gamble', 'invite', 'even', 'create', 'window', 'chance', 'powerful', 'owe', 'perspective', 'sender', 'always', 'john', 'researcher', 'competition', 'mastercard', 'advantage', 'sure', 'bankruptcy', 'paste', 'completely', 'suite', 'much', 'chair', 'european', 'session', 'word', 'pragmatic', 'legitimate', 'teen', 'expiration', 'evidence', 'speak', 'sex', 'august', 'grammatical', 'historical', 'charge', 'mailbox', 'excess', 'city', 'billion', 'lucky', 'modern', 'order', 'summary', 'hesitate', 'programme', 'true', 'search', 'alter', 'enjoy', 'evaluating', 'yahoo', 'miss', 'code', 'tips', 'downline', 'millionaire', 'sentence', 'eliminate', 'trade', 'programs', 'literature', 'clean', 'recruit', 'visit', 'wrap', 'afford', 'sincerely', 'formal', 'are', 'vacation', 'junk', 'truly', 'aim', 'corpus', 'plus', 'secure', 'mci', 'scam', 'mortgage', 'intrusion', 'discuss', 'unsubscribe', 'nl', 'honest', 'deposit', 'quit', 'interaction', 'easiest', 'print', 'most', 'representation', 'away', 'better', 'presentation', 'really', 'datum', 'submit', 'ye', 'signature', 'native', 'relevant', 'academic', 'addresses', 'constraint', 'stop', 'dialect', 'incredible', 'testimonial', 'cleanest', 'overload', 'earnings', 'capitalfm', 'fabulous', 'imagine', 'roll', 'wish', 'description', 'law', 'verify', 'instructions', 'forever', 'society', 'effort', 'shop', 'pick', 'organize', 'registration', 'else', 'id', 'succeed', 'general', 'theme', 'approach', 'job', 'query', 'variety', 'modem', 'hours', 'services', 'worth', 'culture', 'gold', 'software', 'relation', 'phonological', 'filter', 'girl', 'bottom', 'germany', 'ticket', 'astonishment', 'released', 'reg', 'sexually', 'staggering', 'numbers', 'instant', 'totals', 'cram', 'creditor', 'yes', 'game', 'protection', 'mailer', 'sociolinguistic', 'publication', 'trial', 'lexicon', 'family', 'sit', 'phone', 'piece', 'protect', 'pattern', 'argument', 'notification', 'ed', 'beach', 'accurately', 'believer', 'raleigh', 'started', 'vanish', 'release', 'easily', 'interpretation', 'proof', 'let', 'fast', 'instruct', 'speed', 'many', 'movie', 'batch', 'upgrade', 'conceal', 'accountant', 'ordering', 'van', 'financially', 'laugh', 'ez', 'fairchild', 'anytime', 'juno', 'spokane', 'webmaster', 'spout', 'entrepreneur', 'desirous', 'merciless', 'grumble', 'letter', 'book', 'industry', 'forget', 'contribution', 'believe', 'framework', 'cambridge', 'discover', 'present', 'tip', 'doubt', 'please', 'run', 'automatically', 'real', 'proceedings', 'france', 'entire', 'introduction', 'marketer', 'lawful', 'hardcore', 'earth', 'particular', 'name', 'spanish', 'latest', 'allow', 'corporation', 'faith', 'isp', 'fraction', 'limited', 'unsolicit', 'extraordinary', 'someone', 'waste', 'help', 'ours', 'retirement', 'buyer', 'length', 'mit', 'complex', 'et', 'rockland', 'stun', 'making', 'weeks', 'unlimited', 'estate', 'esq', 'pass', 'enterprise', 'noun', 'perfectly', 'rights', 'before', 'alone', 'comparative', 'natural', 'expression', 'isbn', 'welcome', 'advertiser', 'compliance', 'extractor', 'genie', 'variation', 'uk', 'among', 'server', 'did', 'reap', 'days', 'clearance', 'little', 'revenue', 'wall', 'sent', 'amateur', 'penny', 'rom', 'below', 'association', 'owner', 'additional', 'turn', 'read', 'prodigy', 'refinance', 'prepared', 'dupe', 'retail', 'thousands', 'privacy', 'robbery', 'awesome', 'removed', 'phonetic', 'office', 'gov', 'professor', 'download', 'faculty', 'cultural', 'proposal', 'professional', 'alway', 'air', 'client', 'color', 'editor', 'particularly', 'acceptance', 'payable', 'chapter', 'chat', 'march', 'boy', 'magazine', 'dictionary', 'principle', 'qualify', 'japanese', 'mclaughlin', 'savings', 'less', 'percentage', 'whatsoever', 'report', 'chinese', 'type', 'relate', 'increase', 'leave', 'argue', 'au', 'toy', 'rat', 'removal', 'ram', 'trash', 'blvd', 'centre', 'robert', 'dissertation', 'inexpensive', 'potential', 'high', 'article', 'action', 'quickly', 'news', 'exclusive', 'notion', 'morphological', 'reach', 'anything', 'shock', 'classified', 'boyfriend', 'emailer', 'living', 'secrets', 'filled', 'celebrity', 'msn', 'colleague', 'parallel', 'store', 'daily', 'am', 'button', 'subject', 'show', 'february', 'unique', 'discipline', 'distinction', 'lecture', 'cut', 'postage', 'low', 'each', 'blast', 'instantly', 'meg', 'residual', 'benefits', 'casino', 'david', 'skeptical', 'lifetime', 'persistent', 'hold', 'rush', 'bet', 'gift', 'application', 'clause', 'richer', 'goods', 'cds', 'access', 'state', 'logic', 'tremendous', 'convenience', 'provider', 'already', 'lot', 'thing', 'request', 'scholar', 'vowel', 'why', 'der', 'symposium', 'porn', 'wilburn', 'seller', 'inflation', 'someday', 'vulgarity', 'premium', 'pile', 'stamped', 'included', 'proven', 'soon', 'after', 'winner', 'proud', 'file', 'netherland', 'dept', 'sexual', 'golden', 'second', 'until', 'style', 'graduate', 'almost', 'organizer', 'psycholinguistic', 'project', 'recent', 'hand', 'early', 'empirical', 'berlin', 'ready', 'michael', 'hotmail', 'lanse', 'jackson', 'campus', 'immediate', 'participate', 'happen', 'illegal', 'amex', 'cloth', 'ling', 'fastest', 'confidential', 'dial', 'countless', 'embark', 'cable', 'through', 'june', 'ahead', 'jump', 'trust', 'postscript', 'dutch', 'teacher', 'case', 'info', 'move', 'small', 'nc', 'enclose', 'term', 'accommodation', 'consist', 'text', 'test', 'deliver', 'ease', 'exact', 'rather', 'methodology', 'generative', 'jame', 'few', 'operate', 'object', 'catchy', 'downpayment', 'cum', 'recession', 'wisely', 'profanity', 'mega', 'flamer', 'instructed', 'poorer', 'divorce', 'billboard', 'pic', 'biz', 'murkowskus', 'overflow', 'unproductive', 'japan', 'oxford', 'central', 'implication', 'hr', 'model', 'ph', 'phrase', 'publish', 'require', 'define', 'promotion', 'largest', 'stock', 'expensive', 'loan', 'dr', 'obligation', 'thereafter', 'newsgroup', 'bel', 'entertainment', 'martin', 'psychology', 'laboratory', 'hall', 'exp', 'trail', 'postmaster', 'girlfriend', 'affordable', 'unemployment', 'journal', 'september', 'cyber', 'sleep', 'legally', 'recieve', 'gay', 'recipient', 'goodness', 'promptly', 'upset', 'driver', 'awhile', 'continual', 'typology', 'phonetics', 'fr', 'pennsylvanium', 'goal', 'peter', 'carefully', 'obviously', 'scientific', 'average', 'error', 'proceeding', 'derive', 'mark', 'extend', 'verbal', 'asset', 'extremely', 'dialogue', 'preliminary', 'organise', 'biggest', 'human', 'everyday', 'promise', 'november', 'plenary', 'keynote', 'within', 'using', 'drop', 'hypothesis', 'kid', 'various', 'radio', 'husband', 'flame', 'fl', 'pop', 'successful', 'tense', 'originator', 'requesting', 'assuming', 'moving', 'authenticate', 'mba', 'platinum', 'checks', 'commercialemail', 'respectability', 'cards', 'vcs', 'unforeseen', 'selle', 'profits', 'sexiest', 'happy', 'address', 'payment', 'compuserve', 'pronoun', 'accompany', 'edinburgh', 'parse', 'utility', 'record', 'school', 'prof', 'paradise', 'competitor', 'mouse', 'encourage', 'assumption', 'panel', 'spain', 'bibliography', 'convince', 'bit', 'install', 'wealth', 'conservative', 'satisfy', 'star', 'susan', 'scope', 'generally', 'race', 'volume', 'represent', 'correctly', 'manufacturer', 'dare', 'comfort', 'consumer', 'log', 'identify', 'contrast', 'gender', 'reception', 'concern', 'excellent', 'gain', 'erotic', 'exceedingly', 'adults', 'delphus', 'friends', 'favourite', 'anon', 'banner', 'paid', 'apply', 'philosophy', 'en', 'email', 'muncie', 'fortunately', 'criminal', 'permanently', 'album', 'kitchen', 'anonymous', 'insurance', 'mouton', 'univ', 'hard', 'suggest', 'movement', 'function', 'moment', 'box', 'launch', 'highway', 'dave', 'snail', 'md', 'hi', 'patient', 'functional', 'prompt', 'effect', 'north', 'beautiful', 'self', 'dan']\n"
     ]
    }
   ],
   "source": [
    "sorted_ig_df = ig_df.sort_values(by=['ig'], ascending=False)\n",
    "\n",
    "print('\\nDictionary sorted by IG on desending order:')\n",
    "print(sorted_ig_df)\n",
    "\n",
    "top_10_features = sorted_ig_df.head(10)\n",
    "top_100_features = sorted_ig_df.head(100)\n",
    "top_1000_features = sorted_ig_df.head(1000)\n",
    "\n",
    "print(\"\\nTop-10 features:\")\n",
    "print(top_10_features)\n",
    "print(\"\\nTop-100 features:\")\n",
    "print(top_100_features)\n",
    "print(\"\\nTop-1000 features:\")\n",
    "print(top_1000_features)\n",
    "\n",
    "top_10_features_list = [row['word'] for index, row in top_10_features.iterrows()]\n",
    "top_100_features_list = [row['word'] for index, row in top_100_features.iterrows()]\n",
    "top_1000_features_list = [row['word'] for index, row in top_1000_features.iterrows()]\n",
    "\n",
    "print(\"\\nTop-10 words:\")\n",
    "print(top_10_features_list)\n",
    "print(\"\\nTop-100 words:\")\n",
    "print(top_100_features_list)\n",
    "print(\"\\nTop-1000 words:\")\n",
    "print(top_1000_features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21SqfAQJ0m_F"
   },
   "source": [
    "Şimdi ilk 10, ilk 100 ve ilk 1000 olmak üzere 3 özellik sözlüğümüz var. Daha sonra ikili özellik matrislerini veya terim frekans özellik matrislerini bunlara dayalı olarak veri kümelerinden çıkarabiliriz. Hadi başlayalım!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkf3kP9o8NWy"
   },
   "source": [
    "## Bernoulli NB classiﬁer with Binary Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnujaHdg_8zP"
   },
   "source": [
    "Çok değişkenli Bernoulli modelleri için [Bernoulli Naive Bayes sınıflandırıcısı](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html). Bu sınıflandırıcı ayrık veriler için uygundur. BernoulliNB ikili/boolean özellikler için tasarlanmıştır. Burada, bianry özelliği $x_i = $i$ kelimesi bir belgede/e-postada görünüyorsa 1$, aksi halde 0.\n",
    "\n",
    "Daha sonra, daha önce hesaplanan en iyi N özellik sözlüklerine dayalı olarak train setinden/test setinden ikili özellik matrisini çıkaracağız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pPBg-O6CzVZu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "\n",
    "def extract_binary_features(df: DataFrame, N: int):\n",
    "\n",
    "  if N == 10:\n",
    "    top_n_features_list = top_10_features_list\n",
    "  elif N == 100:\n",
    "    top_n_features_list = top_100_features_list\n",
    "  elif N == 1000:\n",
    "    top_n_features_list = top_1000_features_list\n",
    "  else:\n",
    "    print('Please choose a right value for N (10, 100 or 1000)!')\n",
    "    return\n",
    "\n",
    "  assert N == len(top_n_features_list), \"The length of top_n_features_list should be equal with N!\"\n",
    "\n",
    "  features_matrix = np.zeros((df.shape[0], N))\n",
    "  for email_idx, row in df.iterrows():\n",
    "    email_body = row['email_body'].split(' ')\n",
    "    for word_idx in range(len(top_n_features_list)):\n",
    "      word = top_n_features_list[word_idx]\n",
    "      if word in email_body:\n",
    "        features_matrix[email_idx, word_idx] = 1\n",
    "  return features_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cj3VwYnI9kRj"
   },
   "source": [
    "Bir Bernoulli Naive Bayes sınıflandırıcısını ikili özelliklerle eğitin ve test veri setinde kesinlik puanı, geri çağırma puanı elde edin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "674xNHNX9qv0",
    "outputId": "27b6427b-79f6-4193-ebd4-92b69455dba3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB classiﬁer with Binary Features\n",
      "N = 10\n",
      "precision: 0.8888888888888888\n",
      "recall: 0.8163265306122449\n",
      "\n",
      "N = 100\n",
      "precision: 1.0\n",
      "recall: 0.673469387755102\n",
      "\n",
      "N = 1000\n",
      "precision: 1.0\n",
      "recall: 0.6122448979591837\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "print(\"Bernoulli NB classiﬁer with Binary Features\")\n",
    "\n",
    "\n",
    "n = 10\n",
    "x_train_10 = extract_binary_features(lingspam_train_df, n)\n",
    "x_test_10 = extract_binary_features(lingspam_test_df, n)\n",
    "bernoulii_nb_binary_10 = BernoulliNB()\n",
    "bernoulii_nb_binary_10.fit(x_train_10, y_train)\n",
    "\n",
    "y_pred = bernoulii_nb_binary_10.predict(x_test_10)\n",
    "bernoulii_nb_binary_10_precision = precision_score(y_test, y_pred)\n",
    "bernoulii_nb_binary_10_recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"N = {}\".format(n))\n",
    "print(\"precision: {}\".format(bernoulii_nb_binary_10_precision))\n",
    "print(\"recall: {}\\n\".format(bernoulii_nb_binary_10_recall))\n",
    "\n",
    "\n",
    "n = 100\n",
    "x_train_100 = extract_binary_features(lingspam_train_df, n)\n",
    "x_test_100 = extract_binary_features(lingspam_test_df, n)\n",
    "bernoulii_nb_binary_100 = BernoulliNB()\n",
    "bernoulii_nb_binary_100.fit(x_train_100, y_train)\n",
    "\n",
    "y_pred = bernoulii_nb_binary_100.predict(x_test_100)\n",
    "bernoulii_nb_binary_100_precision = precision_score(y_test, y_pred)\n",
    "bernoulii_nb_binary_100_recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"N = {}\".format(n))\n",
    "print(\"precision: {}\".format(bernoulii_nb_binary_100_precision))\n",
    "print(\"recall: {}\\n\".format(bernoulii_nb_binary_100_recall))\n",
    "\n",
    "\n",
    "n = 1000\n",
    "x_train_1000 = extract_binary_features(lingspam_train_df, n)\n",
    "x_test_1000 = extract_binary_features(lingspam_test_df, n)\n",
    "bernoulii_nb_binary_1000 = BernoulliNB()\n",
    "bernoulii_nb_binary_1000.fit(x_train_1000, y_train)\n",
    "\n",
    "y_pred = bernoulii_nb_binary_1000.predict(x_test_1000)\n",
    "bernoulii_nb_binary_1000_precision = precision_score(y_test, y_pred)\n",
    "bernoulii_nb_binary_1000_recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"N = {}\".format(n))\n",
    "print(\"precision: {}\".format(bernoulii_nb_binary_1000_precision))\n",
    "print(\"recall: {}\\n\".format(bernoulii_nb_binary_1000_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRuf0-dc8og0"
   },
   "source": [
    "## Multinomial NB with Binary Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjYeWKZO_9Xn"
   },
   "source": [
    "[Multinomial NB classifier](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) çok terimli modeller içindir. Ayrık özelliklerle sınıflandırma için uygundur (örneğin, metin sınıflandırması için kelime sayısı). Çok terimli dağılım normalde tamsayı özellik sayıları gerektirir. Bu çok terimli NB sınıflandırıcısında ikili özellikleri kullanacağız. İkili özellikleri çıkarma işlemi, önceki sınıflandırıcı ile aynıdır.\n",
    "\n",
    "Daha sonra ikili özelliklere sahip çok terimli bir NB sınıflandırıcıyı eğitin ve modeli değerlendirin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "dDvcmA727hf5",
    "outputId": "8c3953b9-bdd7-4079-a601-0d9c06bc681f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB classiﬁer with Binary Features\n",
      "N = 10\n",
      "precision: 0.8888888888888888\n",
      "recall: 0.8163265306122449\n",
      "\n",
      "N = 100\n",
      "precision: 0.9782608695652174\n",
      "recall: 0.9183673469387755\n",
      "\n",
      "N = 1000\n",
      "precision: 1.0\n",
      "recall: 0.9387755102040817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "print(\"Multinomial NB classiﬁer with Binary Features\")\n",
    "\n",
    "\n",
    "n = 10\n",
    "multinomial_nb_binary_10 = MultinomialNB()\n",
    "multinomial_nb_binary_10.fit(x_train_10, y_train)\n",
    "\n",
    "y_pred = multinomial_nb_binary_10.predict(x_test_10)\n",
    "multinomial_nb_binary_10_precision = precision_score(y_test, y_pred)\n",
    "multinomial_nb_binary_10_recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"N = {}\".format(n))\n",
    "print(\"precision: {}\".format(multinomial_nb_binary_10_precision))\n",
    "print(\"recall: {}\\n\".format(multinomial_nb_binary_10_recall))\n",
    "\n",
    "\n",
    "n = 100\n",
    "multinomial_nb_binary_100 = MultinomialNB()\n",
    "multinomial_nb_binary_100.fit(x_train_100, y_train)\n",
    "\n",
    "y_pred = multinomial_nb_binary_100.predict(x_test_100)\n",
    "multinomial_nb_binary_100_precision = precision_score(y_test, y_pred)\n",
    "multinomial_nb_binary_100_recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"N = {}\".format(n))\n",
    "print(\"precision: {}\".format(multinomial_nb_binary_100_precision))\n",
    "print(\"recall: {}\\n\".format(multinomial_nb_binary_100_recall))\n",
    "\n",
    "\n",
    "n = 1000\n",
    "multinomial_nb_binary_1000 = MultinomialNB()\n",
    "multinomial_nb_binary_1000.fit(x_train_1000, y_train)\n",
    "\n",
    "y_pred = multinomial_nb_binary_1000.predict(x_test_1000)\n",
    "multinomial_nb_binary_1000_precision = precision_score(y_test, y_pred)\n",
    "multinomial_nb_binary_1000_recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"N = {}\".format(n))\n",
    "print(\"precision: {}\".format(multinomial_nb_binary_1000_precision))\n",
    "print(\"recall: {}\\n\".format(multinomial_nb_binary_1000_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfWYw54q8s7j"
   },
   "source": [
    "## Multinomial NB with Term Frequency (TF) Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJXgVq4A_9-L"
   },
   "source": [
    "Bu çok terimli NB sınıflandırıcısında, terim frekansı (TF) özelliklerini kullanacağız. Başka bir deyişle, kelime oluşum sayısını kullanacağız. TF özelliği $x_i$, $i$ teriminin bir belgede/e-postada göründüğü zamanı temsil eder.\n",
    "\n",
    "Ardından, veri kümelerinden TF özelliklerini çıkaracağız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "PwDUKjG98Bfa"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "\n",
    "def extract_tf_features(df: DataFrame, N: int):\n",
    "\n",
    "  if N == 10:\n",
    "    top_n_features_list = top_10_features_list\n",
    "  elif N == 100:\n",
    "    top_n_features_list = top_100_features_list\n",
    "  elif N == 1000:\n",
    "    top_n_features_list = top_1000_features_list\n",
    "  else:\n",
    "    print('Please choose a right value for N (10, 100 or 1000)!')\n",
    "    return\n",
    "\n",
    "  assert N == len(top_n_features_list), \"The length of top_n_features_list should be equal with N!\"\n",
    "\n",
    "  features_matrix = np.zeros((df.shape[0], N))\n",
    "  for email_idx, row in df.iterrows():\n",
    "    email_body = row['email_body'].split(' ')\n",
    "    for word_idx in range(len(top_n_features_list)):\n",
    "      word = top_n_features_list[word_idx]\n",
    "      if word in email_body:\n",
    "        features_matrix[email_idx, word_idx] = email_body.count(word)\n",
    "  return features_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHrWrNsFSfGW"
   },
   "source": [
    "Terim sıklığı özellikleriyle bir Çok Terimli Naive Bayes sınıflandırıcısı eğitin ve test veri kümesinde kesinlik puanı, geri çağırma puanı elde edin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "J_aJio-BSifR",
    "outputId": "f407faac-65ad-4c5b-d72e-6dd9fc57f59c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB classiﬁer with TF Features\n",
      "N = 10\n",
      "precision: 0.8518518518518519\n",
      "recall: 0.9387755102040817\n",
      "\n",
      "N = 100\n",
      "precision: 0.9787234042553191\n",
      "recall: 0.9387755102040817\n",
      "\n",
      "N = 1000\n",
      "precision: 1.0\n",
      "recall: 0.9387755102040817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "print(\"Multinomial NB classiﬁer with TF Features\")\n",
    "\n",
    "\n",
    "n = 10\n",
    "x_train_10 = extract_tf_features(lingspam_train_df, n)\n",
    "x_test_10 = extract_tf_features(lingspam_test_df, n)\n",
    "multinomial_nb_tf_10 = MultinomialNB()\n",
    "multinomial_nb_tf_10.fit(x_train_10, y_train)\n",
    "\n",
    "y_pred = multinomial_nb_tf_10.predict(x_test_10)\n",
    "multinomial_nb_tf_10_precision = precision_score(y_test, y_pred)\n",
    "multinomial_nb_tf_10_recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"N = {}\".format(n))\n",
    "print(\"precision: {}\".format(multinomial_nb_tf_10_precision))\n",
    "print(\"recall: {}\\n\".format(multinomial_nb_tf_10_recall))\n",
    "\n",
    "\n",
    "n = 100\n",
    "x_train_100 = extract_tf_features(lingspam_train_df, n)\n",
    "x_test_100 = extract_tf_features(lingspam_test_df, n)\n",
    "multinomial_nb_tf_100 = MultinomialNB()\n",
    "multinomial_nb_tf_100.fit(x_train_100, y_train)\n",
    "\n",
    "y_pred = multinomial_nb_tf_100.predict(x_test_100)\n",
    "multinomial_nb_tf_100_precision = precision_score(y_test, y_pred)\n",
    "multinomial_nb_tf_100_recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"N = {}\".format(n))\n",
    "print(\"precision: {}\".format(multinomial_nb_tf_100_precision))\n",
    "print(\"recall: {}\\n\".format(multinomial_nb_tf_100_recall))\n",
    "\n",
    "\n",
    "n = 1000\n",
    "x_train_1000 = extract_tf_features(lingspam_train_df, n)\n",
    "x_test_1000 = extract_tf_features(lingspam_test_df, n)\n",
    "multinomial_nb_tf_1000 = MultinomialNB()\n",
    "multinomial_nb_tf_1000.fit(x_train_1000, y_train)\n",
    "\n",
    "y_pred = multinomial_nb_tf_1000.predict(x_test_1000)\n",
    "multinomial_nb_tf_1000_precision = precision_score(y_test, y_pred)\n",
    "multinomial_nb_tf_1000_recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"N = {}\".format(n))\n",
    "print(\"precision: {}\".format(multinomial_nb_tf_1000_precision))\n",
    "print(\"recall: {}\\n\".format(multinomial_nb_tf_1000_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzhKXa9W8yqE"
   },
   "source": [
    "## SVM based Spam Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nG4sU4-H_-kP"
   },
   "source": [
    "Bu bölümde SVM tabanlı bir spam filtresi tasarlayıp uygulayacağım. İlk 1000 özellik, en yüksek Bilgi Kazanımı (IG) puanlarına göre seçilecektir. *extract_tf_features()* işlevinde olduğu gibi TF özelliği seçme yöntemini kullanacağım. Ayrıca, en iyi modeli seçmek için çapraz doğrulama kullanılacaktır.\n",
    "\n",
    "SVM modellerini farklı hiperparametrelerle eğitelim ve performanslarını karşılaştıralım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "z_EzvWtKZWkQ",
    "outputId": "361e3957-fd8b-408e-e43a-60e6ad59cb70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM classiﬁer with TF Features\n",
      "Kernel = linear, C = 1\n",
      "precision: 0.7924528301886793\n",
      "recall: 0.8571428571428571\n",
      "\n",
      "Perform 5-fold Cross Validation on training set:\n",
      "{'fit_time': array([0.1914897 , 0.22140694, 0.21343017, 0.16555762, 0.18251348]), 'score_time': array([0.02194095, 0.02094579, 0.01894903, 0.01695418, 0.01894975]), 'test_precision': array([0.95238095, 0.98795181, 0.94117647, 0.97647059, 0.95294118]), 'test_recall': array([0.91954023, 0.94252874, 0.93023256, 0.96511628, 0.94186047])}\n",
      "best cross validation precision: 0.9879518072289156\n",
      "best cross validation recall: 0.9651162790697675\n",
      "\n",
      "Kernel = linear, C = 2\n",
      "precision: 0.7777777777777778\n",
      "recall: 0.8571428571428571\n",
      "\n",
      "Perform 5-fold Cross Validation on training set:\n",
      "{'fit_time': array([0.22240543, 0.17951965, 0.18051887, 0.19348216, 0.2114346 ]), 'score_time': array([0.02194166, 0.01795268, 0.01695466, 0.01894927, 0.02393603]), 'test_precision': array([0.95238095, 0.97619048, 0.94117647, 0.97647059, 0.96428571]), 'test_recall': array([0.91954023, 0.94252874, 0.93023256, 0.96511628, 0.94186047])}\n",
      "best cross validation precision: 0.9764705882352941\n",
      "best cross validation recall: 0.9651162790697675\n",
      "\n",
      "Kernel = poly, C = 1\n",
      "precision: 1.0\n",
      "recall: 0.14285714285714285\n",
      "\n",
      "Perform 5-fold Cross Validation on training set:\n",
      "{'fit_time': array([0.94946361, 1.01030111, 0.84474158, 0.87266755, 0.82280135]), 'score_time': array([0.18151283, 0.21442699, 0.17653012, 0.20545149, 0.19248819]), 'test_precision': array([0.97142857, 1.        , 1.        , 1.        , 1.        ]), 'test_recall': array([0.3908046 , 0.10344828, 0.24418605, 0.3255814 , 0.25581395])}\n",
      "best cross validation precision: 1.0\n",
      "best cross validation recall: 0.39080459770114945\n",
      "\n",
      "Kernel = poly, C = 2\n",
      "precision: 1.0\n",
      "recall: 0.20408163265306123\n",
      "\n",
      "Perform 5-fold Cross Validation on training set:\n",
      "{'fit_time': array([0.90956926, 0.8766582 , 0.83679342, 0.88762712, 0.8626976 ]), 'score_time': array([0.16156721, 0.15358853, 0.15355825, 0.15857625, 0.15259123]), 'test_precision': array([0.97222222, 1.        , 1.        , 1.        , 1.        ]), 'test_recall': array([0.40229885, 0.17241379, 0.25581395, 0.36046512, 0.26744186])}\n",
      "best cross validation precision: 1.0\n",
      "best cross validation recall: 0.40229885057471265\n",
      "\n",
      "Kernel = rbf, C = 1\n",
      "precision: 1.0\n",
      "recall: 0.673469387755102\n",
      "\n",
      "Perform 5-fold Cross Validation on training set:\n",
      "{'fit_time': array([0.70212388, 0.68915749, 0.65325642, 0.84274936, 0.78390837]), 'score_time': array([0.21542358, 0.21941423, 0.26828098, 0.33111262, 0.27326512]), 'test_precision': array([0.98360656, 1.        , 0.98275862, 1.        , 1.        ]), 'test_recall': array([0.68965517, 0.71264368, 0.6627907 , 0.72093023, 0.74418605])}\n",
      "best cross validation precision: 1.0\n",
      "best cross validation recall: 0.7441860465116279\n",
      "\n",
      "Kernel = rbf, C = 2\n",
      "precision: 1.0\n",
      "recall: 0.7551020408163265\n",
      "\n",
      "Perform 5-fold Cross Validation on training set:\n",
      "{'fit_time': array([0.62133813, 0.60139418, 0.56149936, 0.56748486, 0.57545853]), 'score_time': array([0.19148993, 0.22240496, 0.24634242, 0.25731087, 0.22539806]), 'test_precision': array([0.97014925, 1.        , 0.98387097, 1.        , 1.        ]), 'test_recall': array([0.74712644, 0.7816092 , 0.70930233, 0.76744186, 0.81395349])}\n",
      "best cross validation precision: 1.0\n",
      "best cross validation recall: 0.813953488372093\n",
      "\n",
      "Kernel = sigmoid, C = 1\n",
      "precision: 0.85\n",
      "recall: 0.6938775510204082\n",
      "\n",
      "Perform 5-fold Cross Validation on training set:\n",
      "{'fit_time': array([0.3021934 , 0.37898588, 0.32213664, 0.34208584, 0.33909345]), 'score_time': array([0.03889775, 0.04389501, 0.03590465, 0.05385613, 0.03889656]), 'test_precision': array([0.69662921, 0.72972973, 0.78461538, 0.79746835, 0.71428571]), 'test_recall': array([0.71264368, 0.62068966, 0.59302326, 0.73255814, 0.63953488])}\n",
      "best cross validation precision: 0.7974683544303798\n",
      "best cross validation recall: 0.7325581395348837\n",
      "\n",
      "Kernel = sigmoid, C = 2\n",
      "precision: 0.8571428571428571\n",
      "recall: 0.7346938775510204\n",
      "\n",
      "Perform 5-fold Cross Validation on training set:\n",
      "{'fit_time': array([0.33111477, 0.30019855, 0.27825642, 0.33111525, 0.28025174]), 'score_time': array([0.04388404, 0.03191423, 0.04587722, 0.04488111, 0.03091669]), 'test_precision': array([0.67021277, 0.73684211, 0.71232877, 0.73863636, 0.66666667]), 'test_recall': array([0.72413793, 0.64367816, 0.60465116, 0.75581395, 0.6744186 ])}\n",
      "best cross validation precision: 0.7386363636363636\n",
      "best cross validation recall: 0.7558139534883721\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "print(\"SVM classiﬁer with TF Features\")\n",
    "\n",
    "n = 1000\n",
    "kernel = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "c = [1, 2]\n",
    "\n",
    "x_train = extract_tf_features(lingspam_train_df, n)\n",
    "x_test = extract_tf_features(lingspam_test_df, n)\n",
    "\n",
    "# train svm models\n",
    "for k in kernel:\n",
    "  for reg in c:\n",
    "    svm_model = svm.SVC(kernel=k, C=reg).fit(x_train, y_train)\n",
    "\n",
    "    y_pred = svm_model.predict(x_test)\n",
    "    svm_precision = precision_score(y_test, y_pred)\n",
    "    svm_recall = recall_score(y_test, y_pred)\n",
    "\n",
    "    print(\"Kernel = {}, C = {}\".format(k, reg))\n",
    "    print(\"precision: {}\".format(svm_precision))\n",
    "    print(\"recall: {}\\n\".format(svm_recall))\n",
    "\n",
    "    print(\"Perform 5-fold Cross Validation on training set:\")\n",
    "    scores = cross_validate(\n",
    "        svm_model, \n",
    "        x_train, \n",
    "        y_train, \n",
    "        cv=5,\n",
    "        scoring=('precision', 'recall')\n",
    "        )\n",
    "    print(scores)\n",
    "\n",
    "    cv_best_precision = max(scores['test_precision'])\n",
    "    cv_best_reall = max(scores['test_recall'])\n",
    "    print(\"best cross validation precision: {}\".format(cv_best_precision))\n",
    "    print(\"best cross validation recall: {}\\n\".format(cv_best_reall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNtHpUri9KHx"
   },
   "source": [
    "## Adversarial Classification based Spam Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2HDX_WK__IA"
   },
   "source": [
    "Adversarial Classification, temel bir NB filtresinden kaçmaya çalışan saldırılara yanıt olarak NB tabanlı e-posta spam filtrelerini güncellemeye yönelik bir yaklaşımdır. Bu [paper](https://dl.acm.org/doi/10.1145/1014052.1014066) sunulan teknikleri uygulayacağız. Daha spesifik olarak, aşağıdaki varsayımlar yapılır:\n",
    "\n",
    "* Temel NB sınıflandırıcısı (ikili özelliklere sahip Çok terimli NB), IG metriği ve Boole özellikleri kullanılarak tanımlanan ilk 10 terimi kullanır.\n",
    "* Rakip, KELİME EKLE stratejisini kullanır. Bir kelime eklemenin maliyeti 1'dir. Saldırgan, test setindeki her spam e-postanın temel NB sınıflandırıcı tarafından yasal olarak sınıflandırılacağı minimum maliyetli çözümü bulmaya çalışır.\n",
    "* Düşman, negatif örnekleri değiştiremez, bu da rakibin test setindeki yalnızca spam e-postaları değiştirebileceği anlamına gelir.\n",
    "* Saldırganın yukarıdaki stratejisine yanıt olarak temel NB sınıflandırıcısını güncelleyin. Defans oyuncusu hem yanlış pozitifler hem de yanlış negatifler için bir birim fiyat öder.\n",
    "\n",
    "\n",
    "Saldırganlar e-postaları test etmek için değişiklik yapmadan önce, temel NB sınıflandırıcısının False Negatif oranını karışıklık matrisiyle hesaplayalım. Burada temel model, ikili özelliklere (N=10) sahip multinomial NB'dir. Pozitif sınıfın spam e-posta sınıfı, negatif sınıfın yasal e-posta sınıfı olduğunu varsayalım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "lhrdWMUcUHk1",
    "outputId": "a15b5323-bff0-435b-9fe3-ddfdef1d95e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline nb classifier accuracy rate: 0.9553264604810997, precision: 0.8333333333333334, recall: 0.9183673469387755\n",
      "confusion matrix: \n",
      "[[233   9]\n",
      " [  4  45]]\n",
      "tn: 233, fp: 9, fn: 4, tp: 45\n",
      "fpr: 0.0371900826446281, fnr: 0.08163265306122448\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "n = 10\n",
    "x_train = extract_tf_features(lingspam_train_df, n)\n",
    "x_test = extract_binary_features(lingspam_test_df, n)\n",
    "multinomial_nb_binary_baseline = MultinomialNB()\n",
    "multinomial_nb_binary_baseline.fit(x_train, y_train)\n",
    "\n",
    "y_pred = multinomial_nb_binary_baseline.predict(x_test)\n",
    "\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "conf_mat = confusion_matrix(y_test, y_pred, labels = [0, 1])\n",
    "\n",
    "tn = conf_mat[0][0]\n",
    "fn = conf_mat[1][0]\n",
    "tp = conf_mat[1][1]\n",
    "fp = conf_mat[0][1]\n",
    "# False positive rate\n",
    "fpr = fp / (fp + tn)\n",
    "# False negative rate\n",
    "before_fnr = fn / (tp + fn)\n",
    "\n",
    "print(\"baseline nb classifier accuracy rate: {}, precision: {}, recall: {}\".format(acc_score, precision, recall))\n",
    "print(\"confusion matrix: \\n{}\".format(conf_mat))\n",
    "print(\"tn: {}, fp: {}, fn: {}, tp: {}\".format(tn, fp, fn, tp))\n",
    "print(\"fpr: {}, fnr: {}\".format(fpr, before_fnr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrNuJrugrNMb"
   },
   "source": [
    "İlk olarak, test setindeki tüm spam e-postaları alalım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "MgBuAYZlK31O",
    "outputId": "3da8cd90-7592-424c-a983-a6de6962a5b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam email size: 49\n",
      "spam email list: [62, 63, 64, 68, 83, 84, 85, 87, 91, 92, 93, 109, 111, 112, 113, 114, 127, 128, 129, 130, 131, 146, 148, 149, 153, 154, 169, 170, 173, 176, 177, 190, 191, 193, 194, 195, 196, 197, 206, 213, 214, 215, 216, 217, 219, 220, 237, 238, 240]\n"
     ]
    }
   ],
   "source": [
    "spam_email_list = []\n",
    "\n",
    "for email_idx, row in lingspam_test_df.iterrows():\n",
    "  if row['is_spam'] == 1:\n",
    "    spam_email_list.append(email_idx)\n",
    "\n",
    "spam_email_size = len(spam_email_list)\n",
    "print(\"spam email size: {}\".format(spam_email_size))\n",
    "print(\"spam email list: {}\".format(spam_email_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0KX8qcqGffU"
   },
   "source": [
    "Ardından, bu spam e-postalarda düşman için KELİME EKLE stratejisini uygulayın. Saldırgan, test setindeki her spam e-postanın temel NB sınıflandırıcısı tarafından meşru olarak sınıflandırılacağı şekilde minimum maliyetli çözümü bulmaya çalışır. Sınıflandırıcıyı kandırabilmeleri için test setindeki spam e-postalara kelimeler ekleyeceğiz. Sözcükler, IG metriğine göre sıralanan ilk 10 terim arasından seçilir. KELİME EKLE stratejisi, burada açgözlü bir algoritma gibidir, bir spam e-postanın tahmin sonucu tersine döndüğünde, kelime eklemeyi bırakın.\n",
    "\n",
    "Bir kelime eklemenin maliyeti 1'dir. Test setindeki tüm spam e-postaların ortalamasını alarak, saldırganın değişikliklerinin ortalama maliyetini hesaplayacağız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "R2B4T-CmriWc",
    "outputId": "4aab7db9-f383-4c8e-df1f-e336e0b31d2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing ADD-WORDS strategy on testset...\n",
      "email index: 62\n",
      "original feature matrix: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "modified featurex matrix: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "cost: 0\n",
      "email index: 63\n",
      "original feature matrix: [[0. 1. 1. 0. 0. 0. 1. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 0. 0. 1. 0. 1. 0.]]\n",
      "cost: 2\n",
      "email index: 64\n",
      "original feature matrix: [[0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 0. 0. 0. 0. 0. 1. 0. 1. 0.]]\n",
      "cost: 1\n",
      "email index: 68\n",
      "original feature matrix: [[0. 0. 1. 0. 0. 0. 0. 0. 1. 1.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 0. 0. 0. 0. 1. 1.]]\n",
      "cost: 3\n",
      "email index: 83\n",
      "original feature matrix: [[0. 0. 1. 0. 0. 0. 0. 0. 1. 1.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 0. 0. 0. 0. 1. 1.]]\n",
      "cost: 3\n",
      "email index: 84\n",
      "original feature matrix: [[0. 0. 1. 0. 0. 1. 1. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 1. 1. 1. 0. 1. 0.]]\n",
      "cost: 4\n",
      "email index: 85\n",
      "original feature matrix: [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "modified featurex matrix: [[1. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "cost: 1\n",
      "email index: 87\n",
      "original feature matrix: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "modified featurex matrix: [[1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "cost: 1\n",
      "email index: 91\n",
      "original feature matrix: [[0. 1. 1. 0. 0. 0. 1. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 0. 0. 1. 0. 1. 0.]]\n",
      "cost: 2\n",
      "email index: 92\n",
      "original feature matrix: [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "cost: 1\n",
      "email index: 93\n",
      "original feature matrix: [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "modified featurex matrix: [[1. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "cost: 1\n",
      "email index: 109\n",
      "original feature matrix: [[0. 0. 0. 0. 0. 1. 0. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 0. 0. 0. 0. 1. 0. 0. 1. 0.]]\n",
      "cost: 1\n",
      "email index: 111\n",
      "original feature matrix: [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "cost: 1\n",
      "email index: 112\n",
      "original feature matrix: [[0. 0. 1. 0. 0. 1. 0. 1. 1. 1.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "cost: 5\n",
      "email index: 113\n",
      "original feature matrix: [[0. 0. 0. 0. 1. 0. 0. 0. 1. 0.]]\n",
      "modified featurex matrix: [[0. 0. 0. 0. 1. 0. 0. 0. 1. 0.]]\n",
      "cost: 0\n",
      "email index: 114\n",
      "original feature matrix: [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "cost: 1\n",
      "email index: 127\n",
      "original feature matrix: [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "modified featurex matrix: [[1. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "cost: 1\n",
      "email index: 128\n",
      "original feature matrix: [[0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "cost: 2\n",
      "email index: 129\n",
      "original feature matrix: [[0. 0. 1. 0. 0. 1. 1. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 1. 1. 1. 0. 1. 0.]]\n",
      "cost: 4\n",
      "email index: 130\n",
      "original feature matrix: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "modified featurex matrix: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "cost: 0\n",
      "email index: 131\n",
      "original feature matrix: [[0. 0. 1. 0. 0. 0. 1. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 0. 0. 1. 0. 1. 0.]]\n",
      "cost: 3\n",
      "email index: 146\n",
      "original feature matrix: [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "cost: 1\n",
      "email index: 148\n",
      "original feature matrix: [[0. 0. 1. 0. 0. 1. 1. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 1. 1. 1. 0. 1. 0.]]\n",
      "cost: 4\n",
      "email index: 149\n",
      "original feature matrix: [[0. 1. 1. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 0. 0. 0. 0. 1. 0.]]\n",
      "cost: 2\n",
      "email index: 153\n",
      "original feature matrix: [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "modified featurex matrix: [[1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "cost: 1\n",
      "email index: 154\n",
      "original feature matrix: [[0. 1. 1. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 0. 0. 0. 0. 1. 0.]]\n",
      "cost: 2\n",
      "email index: 169\n",
      "original feature matrix: [[0. 0. 1. 0. 0. 1. 0. 1. 1. 1.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "cost: 5\n",
      "email index: 170\n",
      "original feature matrix: [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "modified featurex matrix: [[1. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "cost: 1\n",
      "email index: 173\n",
      "original feature matrix: [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "cost: 1\n",
      "email index: 176\n",
      "original feature matrix: [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "modified featurex matrix: [[1. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "cost: 1\n",
      "email index: 177\n",
      "original feature matrix: [[0. 1. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 0. 0. 1. 0. 0. 0.]]\n",
      "cost: 3\n",
      "email index: 190\n",
      "original feature matrix: [[0. 1. 1. 0. 0. 1. 0. 1. 1. 1.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "cost: 4\n",
      "email index: 191\n",
      "original feature matrix: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "modified featurex matrix: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "cost: 0\n",
      "email index: 193\n",
      "original feature matrix: [[0. 1. 1. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 0. 0. 0. 0. 1. 0.]]\n",
      "cost: 2\n",
      "email index: 194\n",
      "original feature matrix: [[0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 1. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "cost: 1\n",
      "email index: 195\n",
      "original feature matrix: [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "modified featurex matrix: [[1. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "cost: 1\n",
      "email index: 196\n",
      "original feature matrix: [[0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 0. 0. 0. 0. 0. 1. 0. 1. 0.]]\n",
      "cost: 1\n",
      "email index: 197\n",
      "original feature matrix: [[0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 1. 1. 1. 0. 0. 0.]]\n",
      "cost: 5\n",
      "email index: 206\n",
      "original feature matrix: [[0. 1. 1. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 0. 0. 0. 0. 0. 1.]]\n",
      "cost: 2\n",
      "email index: 213\n",
      "original feature matrix: [[0. 0. 1. 0. 0. 0. 1. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 0. 0. 1. 0. 1. 0.]]\n",
      "cost: 3\n",
      "email index: 214\n",
      "original feature matrix: [[0. 0. 0. 0. 0. 1. 1. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 1. 1. 1. 0. 1. 0.]]\n",
      "cost: 5\n",
      "email index: 215\n",
      "original feature matrix: [[0. 0. 1. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 0. 0. 1. 0. 0. 0.]]\n",
      "cost: 3\n",
      "email index: 216\n",
      "original feature matrix: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "modified featurex matrix: [[1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "cost: 1\n",
      "email index: 217\n",
      "original feature matrix: [[0. 0. 1. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 0. 1. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "cost: 1\n",
      "email index: 219\n",
      "original feature matrix: [[0. 1. 1. 0. 0. 0. 1. 1. 1. 1.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "cost: 4\n",
      "email index: 220\n",
      "original feature matrix: [[0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 1. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "cost: 1\n",
      "email index: 237\n",
      "original feature matrix: [[0. 1. 0. 0. 0. 0. 1. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 0. 0. 1. 0. 1. 0.]]\n",
      "cost: 3\n",
      "email index: 238\n",
      "original feature matrix: [[0. 1. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 0. 0. 1. 0. 0. 0.]]\n",
      "cost: 3\n",
      "email index: 240\n",
      "original feature matrix: [[0. 0. 1. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 0. 1. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "cost: 1\n",
      "all cost on testset: 99\n",
      "average cost: 2.020408163265306\n"
     ]
    }
   ],
   "source": [
    "print('performing ADD-WORDS strategy on testset...')\n",
    "all_cost = 0\n",
    "modified_x_test = x_test\n",
    "\n",
    "for email_idx, row in lingspam_test_df.iterrows():\n",
    "  if email_idx in spam_email_list:\n",
    "    cost = 0\n",
    "    print('email index: {}'.format(email_idx))\n",
    "    features_matrix = np.zeros((1, 10))\n",
    "    email_body = row['email_body'].split(' ')\n",
    "    for word_idx in range(len(top_10_features_list)):\n",
    "      word = top_10_features_list[word_idx]\n",
    "      if word in email_body:\n",
    "        features_matrix[0, word_idx] = 1\n",
    "    \n",
    "    print('original feature matrix: {}'.format(features_matrix))\n",
    "\n",
    "    while multinomial_nb_binary_baseline.predict(features_matrix) != 0:\n",
    "      idx = next((i for i, x in enumerate(features_matrix[0]) if x == 0), None)\n",
    "      if idx == None:\n",
    "        break\n",
    "      features_matrix[0, idx] = 1\n",
    "      cost += 1\n",
    "    \n",
    "    all_cost += cost\n",
    "    modified_x_test[email_idx] = features_matrix\n",
    "    print('modified featurex matrix: {}'.format(features_matrix))\n",
    "    print('cost: {}'.format(cost))\n",
    "  \n",
    "avg_cost = all_cost / spam_email_size\n",
    "print('all cost on testset: {}'.format(all_cost))\n",
    "print('average cost: {}'.format(avg_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bB0SG1WNxpA2"
   },
   "source": [
    "Değiştirilmiş test setinde temel sınıflandırıcıyı değerlendirin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "id": "obqnwbzi1Gcg",
    "outputId": "996fa8f9-8935-4f5b-b0cc-d89b4076055d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After the attacker's modifications to test emails\n",
      "baseline nb classifier accuracy rate: 0.8144329896907216, precision: 0.3076923076923077, recall: 0.08163265306122448\n",
      "confusion matrix: \n",
      "[[233   9]\n",
      " [ 45   4]]\n",
      "tn: 233, fp: 9, fn: 45, tp: 4\n",
      "fpr: 0.0371900826446281, fnr: 0.9183673469387755\n"
     ]
    }
   ],
   "source": [
    "after_y_pred = multinomial_nb_binary_baseline.predict(modified_x_test)\n",
    "\n",
    "acc_score = accuracy_score(y_test, after_y_pred)\n",
    "precision = precision_score(y_test, after_y_pred)\n",
    "recall = recall_score(y_test, after_y_pred)\n",
    "conf_mat = confusion_matrix(y_test, after_y_pred, labels = [0, 1])\n",
    "\n",
    "tn = conf_mat[0][0]\n",
    "fn = conf_mat[1][0]\n",
    "tp = conf_mat[1][1]\n",
    "fp = conf_mat[0][1]\n",
    "# False positive rate\n",
    "fpr = fp / (fp + tn)\n",
    "# False negative rate\n",
    "after_fnr = fn / (tp + fn)\n",
    "\n",
    "print(\"After the attacker's modifications to test emails\")\n",
    "print(\"baseline nb classifier accuracy rate: {}, precision: {}, recall: {}\".format(acc_score, precision, recall))\n",
    "print(\"confusion matrix: \\n{}\".format(conf_mat))\n",
    "print(\"tn: {}, fp: {}, fn: {}, tp: {}\".format(tn, fp, fn, tp))\n",
    "print(\"fpr: {}, fnr: {}\".format(fpr, after_fnr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1H9aQjNRLZRp"
   },
   "source": [
    "Eğitim setinde aynı ADD-WORDS stratejisini uygulayın, ardından temel sınıflandırıcıyı yeniden eğitin ve güncellenen temel sınıflandırıcıyı değerlendirin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "UqF-f_guMBlw",
    "outputId": "73b892bd-c5fe-4b52-bef5-3b00f06e4339"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam email size: 432\n",
      "spam email list: [21, 38, 84, 85, 86, 92, 93, 94, 103, 104, 111, 112, 116, 117, 132, 133, 137, 138, 139, 153, 154, 155, 157, 158, 159, 160, 168, 169, 170, 171, 178, 179, 180, 187, 189, 190, 198, 199, 208, 209, 212, 213, 214, 228, 229, 231, 278, 281, 354, 359, 360, 361, 362, 370, 374, 375, 384, 385, 386, 387, 401, 402, 404, 405, 431, 432, 434, 436, 444, 445, 446, 447, 457, 458, 459, 460, 471, 472, 473, 474, 482, 483, 484, 485, 494, 495, 496, 497, 507, 508, 509, 510, 513, 514, 515, 516, 582, 583, 591, 593, 605, 606, 614, 616, 625, 626, 636, 637, 647, 649, 654, 656, 666, 668, 671, 677, 678, 684, 689, 693, 694, 700, 705, 709, 713, 719, 720, 729, 733, 735, 755, 764, 774, 785, 803, 805, 814, 815, 838, 849, 854, 856, 866, 867, 904, 918, 920, 923, 939, 942, 952, 956, 958, 968, 969, 971, 972, 986, 988, 990, 1002, 1003, 1016, 1017, 1018, 1036, 1039, 1040, 1053, 1054, 1059, 1061, 1064, 1073, 1075, 1076, 1080, 1081, 1092, 1094, 1098, 1099, 1101, 1117, 1118, 1119, 1125, 1126, 1135, 1136, 1144, 1151, 1157, 1174, 1189, 1198, 1202, 1205, 1218, 1242, 1250, 1254, 1268, 1272, 1281, 1285, 1286, 1295, 1296, 1306, 1307, 1308, 1309, 1319, 1320, 1326, 1327, 1328, 1330, 1344, 1345, 1363, 1365, 1366, 1367, 1387, 1388, 1389, 1390, 1391, 1403, 1404, 1407, 1408, 1426, 1427, 1428, 1430, 1444, 1445, 1448, 1453, 1454, 1456, 1459, 1460, 1462, 1474, 1476, 1477, 1479, 1481, 1482, 1499, 1500, 1503, 1505, 1506, 1508, 1519, 1520, 1523, 1524, 1581, 1591, 1605, 1621, 1629, 1666, 1669, 1670, 1671, 1672, 1675, 1692, 1693, 1694, 1696, 1709, 1711, 1713, 1714, 1722, 1726, 1727, 1728, 1729, 1730, 1736, 1737, 1740, 1741, 1756, 1758, 1759, 1761, 1765, 1766, 1769, 1783, 1785, 1786, 1791, 1792, 1793, 1804, 1805, 1806, 1807, 1808, 1827, 1828, 1829, 1908, 1919, 1922, 1937, 1938, 1939, 1953, 1954, 1956, 1960, 1961, 1979, 1980, 1984, 1986, 1996, 1997, 2001, 2002, 2003, 2018, 2019, 2021, 2060, 2063, 2068, 2079, 2090, 2105, 2108, 2122, 2125, 2129, 2132, 2143, 2156, 2157, 2162, 2163, 2179, 2180, 2181, 2183, 2185, 2186, 2195, 2196, 2200, 2201, 2216, 2217, 2221, 2222, 2235, 2238, 2242, 2244, 2258, 2261, 2264, 2267, 2268, 2271, 2283, 2284, 2291, 2292, 2300, 2301, 2306, 2307, 2317, 2318, 2319, 2320, 2321, 2322, 2333, 2335, 2345, 2346, 2347, 2348, 2365, 2373, 2374, 2376, 2379, 2392, 2395, 2396, 2397, 2399, 2400, 2413, 2505, 2507, 2527, 2529, 2545, 2546, 2547, 2548, 2549, 2550, 2559, 2560, 2561, 2562, 2575, 2576, 2577, 2579, 2591, 2593, 2595, 2596, 2597, 2598]\n",
      "performing ADD-WORDS strategy on trainset...\n",
      "all cost on trainset: 1043\n",
      "average cost: 2.4143518518518516\n",
      "Updateing baseline classifier...\n",
      "baseline nb classifier accuracy rate: 0.865979381443299, precision: 0.5806451612903226, recall: 0.7346938775510204\n",
      "confusion matrix: \n",
      "[[216  26]\n",
      " [ 13  36]]\n",
      "tn: 216, fp: 26, fn: 13, tp: 36\n",
      "fpr: 0.10743801652892562, fnr: 0.2653061224489796\n"
     ]
    }
   ],
   "source": [
    "train_spam_email_list = []\n",
    "\n",
    "for email_idx, row in lingspam_train_df.iterrows():\n",
    "  if row['is_spam'] == 1:\n",
    "    train_spam_email_list.append(email_idx)\n",
    "\n",
    "train_spam_email_size = len(train_spam_email_list)\n",
    "print(\"spam email size: {}\".format(train_spam_email_size))\n",
    "print(\"spam email list: {}\".format(train_spam_email_list))\n",
    "\n",
    "\n",
    "print('performing ADD-WORDS strategy on trainset...')\n",
    "all_cost_train = 0\n",
    "modified_x_train = x_train\n",
    "\n",
    "for email_idx, row in lingspam_train_df.iterrows():\n",
    "  if email_idx in train_spam_email_list:\n",
    "    cost = 0\n",
    "    # print('email index: {}'.format(email_idx))\n",
    "    features_matrix = np.zeros((1, 10))\n",
    "    email_body = row['email_body'].split(' ')\n",
    "    for word_idx in range(len(top_10_features_list)):\n",
    "      word = top_10_features_list[word_idx]\n",
    "      if word in email_body:\n",
    "        features_matrix[0, word_idx] = 1\n",
    "    \n",
    "    # print('original feature matrix: {}'.format(features_matrix))\n",
    "\n",
    "    while multinomial_nb_binary_baseline.predict(features_matrix) != 0:\n",
    "      idx = next((i for i, x in enumerate(features_matrix[0]) if x == 0), None)\n",
    "      if idx == None:\n",
    "        break\n",
    "      features_matrix[0, idx] = 1\n",
    "      cost += 1\n",
    "    \n",
    "    all_cost_train += cost\n",
    "    modified_x_train[email_idx] = features_matrix\n",
    "    # print('modified featurex matrix: {}'.format(features_matrix))\n",
    "    # print('cost: {}'.format(cost))\n",
    "  \n",
    "avg_cost_train = all_cost_train / train_spam_email_size\n",
    "print('all cost on trainset: {}'.format(all_cost_train))\n",
    "print('average cost: {}'.format(avg_cost_train))\n",
    "\n",
    "print('Updateing baseline classifier...')\n",
    "multinomial_nb_binary_updated = MultinomialNB()\n",
    "multinomial_nb_binary_updated.fit(modified_x_train, y_train)\n",
    "\n",
    "updated_y_pred = multinomial_nb_binary_updated.predict(modified_x_test)\n",
    "\n",
    "acc_score = accuracy_score(y_test, updated_y_pred)\n",
    "precision = precision_score(y_test, updated_y_pred)\n",
    "recall = recall_score(y_test, updated_y_pred)\n",
    "conf_mat = confusion_matrix(y_test, updated_y_pred, labels = [0, 1])\n",
    "\n",
    "tn = conf_mat[0][0]\n",
    "fn = conf_mat[1][0]\n",
    "tp = conf_mat[1][1]\n",
    "fp = conf_mat[0][1]\n",
    "# False positive rate\n",
    "updated_fpr = fp / (fp + tn)\n",
    "# False negative rate\n",
    "updated_fnr = fn / (tp + fn)\n",
    "\n",
    "print(\"baseline nb classifier accuracy rate: {}, precision: {}, recall: {}\".format(acc_score, precision, recall))\n",
    "print(\"confusion matrix: \\n{}\".format(conf_mat))\n",
    "print(\"tn: {}, fp: {}, fn: {}, tp: {}\".format(tn, fp, fn, tp))\n",
    "print(\"fpr: {}, fnr: {}\".format(updated_fpr, updated_fnr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2dhRnLe9Xe2"
   },
   "source": [
    "# Sonuç"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTYqaCDG__2A"
   },
   "source": [
    "Naive Bayes sınıflandırıcılarının değerlendirme sonuçları aşağıdaki tabloda yer almaktadır. Her sınıflandırıcı ve N kombinasyon için bir tane olmak üzere 9 satırı vardır ($N = {10, 100, 1000}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMGx2rIR9fKH"
   },
   "source": [
    "Naive Bayes Classifier | N | Precision | Recall\n",
    "--- | --- | --- | ---\n",
    "Bernoulli NB classiﬁer with binary features | 10 | 0.8888888888888888 | 0.8163265306122449\n",
    "Bernoulli NB classiﬁer with binary features | 100 | 1.0 | 0.673469387755102\n",
    "Bernoulli NB classiﬁer with binary features | 1000 | 1.0 | 0.6122448979591837\n",
    "Multinomial NB with binary features | 10 | 0.8888888888888888 | 0.8163265306122449\n",
    "Multinomial NB with binary features | 100 | 0.9782608695652174 | 0.9183673469387755\n",
    "Multinomial NB with binary features | 1000 | 1.0 | 0.9387755102040817\n",
    "Multinomial NB with term frequency (TF) features | 10 | 0.8518518518518519 | 0.9387755102040817\n",
    "Multinomial NB with term frequency (TF) features | 100 | 0.9787234042553191 | 0.9387755102040817\n",
    "Multinomial NB with term frequency (TF) features | 1000 | 1.0 | 0.9387755102040817\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mK1r-n8qL8J1"
   },
   "source": [
    "Farklı hiperparametrelere sahip SVM sınıflandırıcılarının değerlendirme sonuçları aşağıdaki tabloda yer almaktadır. 4 farklı çekirdek test edilir, bunlar ['linear', 'poly', 'rbf', 'sigmoid']. $C$, düzenlileştirme parametresini temsil eder. Burada, $C \\içinde [1, 2]$. Düzenlemenin gücü $C$ ile ters orantılıdır. SVM sınıflandırıcıları, bilgi kazancına göre seçilen en iyi 1000 özelliği kullanır ve terim frekans özelliklerini kullanır. Ayrıca eğitim setinde 5 kat çapraz doğrulama yapılır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjCmSF0tMejy"
   },
   "source": [
    "SVM Classifier | Kernel | C | Cross Validation | Precision | Recall\n",
    "--- | --- | --- | --- | --- | ---\n",
    "SVM classifier with TF features | linear | 1 | False | 0.7924528301886793 | 0.8571428571428571\n",
    "SVM classifier with TF features | linear | 1 | True | 0.9879518072289156 | 0.9651162790697675\n",
    "SVM classifier with TF features | linear | 2 | False | 0.7777777777777778 | 0.8571428571428571\n",
    "SVM classifier with TF features | linear | 2 | True | 0.9764705882352941 | 0.9651162790697675\n",
    "SVM classifier with TF features | poly | 1 | False | 1.0 | 0.14285714285714285\n",
    "SVM classifier with TF features | poly | 1 | True | 1.0 | 0.39080459770114945\n",
    "SVM classifier with TF features | poly | 2 | False | 1.0 | 0.20408163265306123\n",
    "SVM classifier with TF features | poly | 2 | True | 1.0 | 0.40229885057471265\n",
    "SVM classifier with TF features | rbf | 1 | False | 1.0 | 0.673469387755102\n",
    "SVM classifier with TF features | rbf | 1 | True | 1.0 | 0.7441860465116279\n",
    "SVM classifier with TF features | rbf | 2 | False | 1.0 | 0.7551020408163265\n",
    "SVM classifier with TF features | rbf | 2 | True | 1.0 | 0.813953488372093\n",
    "SVM classifier with TF features | sigmoid | 1 | False | 0.85 | 0.6938775510204082\n",
    "SVM classifier with TF features | sigmoid | 1 | True | 0.7974683544303798 | 0.7325581395348837\n",
    "SVM classifier with TF features | sigmoid | 2 | False | 0.8571428571428571 | 0.7346938775510204\n",
    "SVM classifier with TF features | sigmoid | 2 | True | 0.7386363636363636 | 0.7558139534883721"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LOVOoesT1Rd"
   },
   "source": [
    "Adversarial Attack sınıflandırıcılarının değerlendirme sonuçları aşağıdaki gibidir. Burada temel NB sınıflandırıcısı, ikili özelliklere sahip çok terimli NB'dir (N=10, ilk 10 özellik seçilir)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "30ui54Bf47rB",
    "outputId": "52453ddc-983b-420f-b3a8-7f35d4654e94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Negative Rate of the baseline NB classifier before attacker's modification: 0.08163265306122448\n",
      "False Negative Rate of the baseline NB classifier after attacker's modification: 0.9183673469387755\n",
      "Average cost of attacker's modifications: 2.020408163265306\n",
      "False Negative Rate of updated NB classifier: 0.2653061224489796\n",
      "False Positvive Rate of updated NB classifier: 0.10743801652892562\n"
     ]
    }
   ],
   "source": [
    "print('False Negative Rate of the baseline NB classifier before attacker\\'s modification: {}'.format(before_fnr))\n",
    "print('False Negative Rate of the baseline NB classifier after attacker\\'s modification: {}'.format(after_fnr))\n",
    "print('Average cost of attacker\\'s modifications: {}'.format(avg_cost))\n",
    "print('False Negative Rate of updated NB classifier: {}'.format(updated_fnr))\n",
    "print('False Positvive Rate of updated NB classifier: {}'.format(updated_fpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "email_spam_filter.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
